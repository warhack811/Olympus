"""
FALLBACK MODEL TEST
-------------------
Goal: Verify moonshotai/kimi-k2-instruct works when primary fails
Method: Force primary model failure, observe fallback
"""

import sys
sys.path.insert(0, 'd:/ai/mami_ai_v4')

import asyncio
from unittest.mock import patch, AsyncMock


async def test_fallback_model():
    """Test fallback to moonshotai when llama-3.3-70b fails."""
    print("\n" + "="*80)
    print("üß™ FALLBACK MODEL TEST - moonshotai/kimi-k2-instruct")
    print("="*80)
    
    from app.services.brain.task_runner import task_runner
    from app.services.brain.intent import TaskSpec
    from app.providers.llm.groq import GroqProvider
    
    # Create synthesis task
    task = TaskSpec(
        id="t1",
        type="generation",
        specialist="logic",
        instruction="Python nedir kƒ±saca a√ßƒ±kla",
        dependencies=[]
    )
    
    print("\nüìã Test Scenario:")
    print("   1. PRIMARY model: llama-3.3-70b-versatile (WILL FAIL)")
    print("   2. FALLBACK model: moonshotai/kimi-k2-instruct (SHOULD WORK)")
    
    # Track which models were called
    call_log = []
    
    original_generate = GroqProvider.generate
    
    async def mock_generate(self, **kwargs):
        """Mock that fails on primary, succeeds on fallback."""
        model = kwargs.get('model', 'unknown')
        call_log.append(model)
        
        print(f"\n   üîÑ Attempting model: {model}")
        
        # Simulate primary failure
        if model == "llama-3.3-70b-versatile":
            print(f"      ‚ùå Simulated failure (primary)")
            raise Exception("Simulated API rate limit")
        
        # Fallback succeeds
        elif "moonshotai" in model or "kimi" in model:
            print(f"      ‚úÖ Fallback model working")
            return """<thought>Fallback model kullanarak Python'u a√ßƒ±klƒ±yorum.</thought>
Python, y√ºksek seviyeli bir programlama dilidir."""
        
        # Other models
        else:
            print(f"      ‚ö†Ô∏è Unexpected model: {model}")
            return await original_generate(self, **kwargs)
    
    # Patch GroqProvider.generate
    with patch.object(GroqProvider, 'generate', mock_generate):
        print("\n‚è≥ Running synthesis with fallback simulation...")
        
        try:
            result = await task_runner._execute_generation(
                task=task,
                intent="general",
                executed_tasks={},
                original_message="Python nedir?",
                session_id="test_fallback",
                user_id="test_user"
            )
            
            print("\n" + "="*80)
            print("üìä TEST RESULTS")
            print("="*80)
            
            # Analyze call log
            print(f"\nüìù Model Call Sequence:")
            for idx, model in enumerate(call_log, 1):
                status = "‚ùå FAILED" if model == "llama-3.3-70b-versatile" else "‚úÖ SUCCESS"
                print(f"   {idx}. {model}: {status}")
            
            # Verify results
            model_used = result.get('model', 'UNKNOWN')
            thought = result.get('thought', '')
            output = result.get('output', '')
            status = result.get('status', 'UNKNOWN')
            
            print(f"\nüí¨ Final Response:")
            print(f"   Model Used: {model_used}")
            print(f"   Status: {status}")
            print(f"   Thought: \"{thought[:80]}...\"" if len(thought) > 80 else f"   Thought: \"{thought}\"")
            print(f"   Output: \"{output[:80]}...\"" if len(output) > 80 else f"   Output: \"{output}\"")
            
            # Verification
            print(f"\nüîç Verification:")
            
            # Check primary was attempted
            primary_attempted = "llama-3.3-70b-versatile" in call_log
            print(f"   PRIMARY attempted: {'‚úÖ YES' if primary_attempted else '‚ùå NO'}")
            
            # Check fallback was used
            fallback_used = any("moonshotai" in m or "kimi" in m for m in call_log)
            print(f"   FALLBACK used: {'‚úÖ YES' if fallback_used else '‚ùå NO'}")
            
            # Check final model is fallback
            is_fallback_model = "moonshotai" in model_used or "kimi" in model_used
            print(f"   Final model is FALLBACK: {'‚úÖ YES' if is_fallback_model else f'‚ùå NO ({model_used})'}")
            
            # Check response exists
            has_response = len(output) > 10
            print(f"   Response generated: {'‚úÖ YES' if has_response else '‚ùå NO'}")
            
            # Final verdict
            print("\n" + "="*80)
            if primary_attempted and fallback_used and is_fallback_model and has_response:
                print("‚úÖ FALLBACK TEST PASSED")
                print("="*80)
                print("\nüí° Conclusion:")
                print("   - PRIMARY model failed (simulated)")
                print("   - FALLBACK model took over")
                print("   - Response generated successfully")
                print("   - ModelGovernance chain working correctly")
            else:
                print("‚ùå FALLBACK TEST FAILED")
                print("="*80)
                if not fallback_used:
                    print("\n‚ö†Ô∏è Issue: Fallback model was NOT used")
                if not is_fallback_model:
                    print(f"\n‚ö†Ô∏è Issue: Final model is not fallback: {model_used}")
            
        except Exception as e:
            print(f"\n‚ùå TEST ERROR: {e}")
            import traceback
            traceback.print_exc()


async def test_end_to_end_flow():
    """Test complete flow: Router ‚Üí Orchestrator ‚Üí DAG ‚Üí Synthesis."""
    print("\n" + "="*80)
    print("üß™ END-TO-END USER FLOW TEST")
    print("="*80)
    
    print("\nüìã Test Scenario:")
    print("   User Input: 'Python programlama dili nedir?'")
    print("   Expected Flow:")
    print("      1. Smart Router (intent detection)")
    print("      2. Orchestrator (DAG planning)")
    print("      3. Task Runner (execute plan)")
    print("      4. Synthesis (embedded thought)")
    
    print("\n‚ö†Ô∏è Note: This requires full system integration")
    print("   Skipping for now - requires running backend")
    
    # TODO: Implement when backend is running
    # from app.chat.smart_router import smart_router
    # async for chunk in smart_router.route_stream(...):
    #     print(chunk)


if __name__ == "__main__":
    print("\n" + "üöÄ"*40)
    print("NEXT STEP: FALLBACK MODEL VERIFICATION")
    print("üöÄ"*40)
    
    # Run fallback test
    asyncio.run(test_fallback_model())
    
    # End-to-end test (placeholder)
    asyncio.run(test_end_to_end_flow())
