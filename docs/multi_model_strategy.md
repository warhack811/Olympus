# Mami AI: Multi-Model Architecture & Orchestration Strategy

**Durum:** Draft Proposal  
**Hedef:** Groq API Ã¼zerindeki baÄŸÄ±msÄ±z rate limitleri kullanarak proje performansÄ±nÄ± ve dayanÄ±klÄ±lÄ±ÄŸÄ±nÄ± (resilience) maksimize etmek.

---

## 1. Mimari Felsefesi: "Right Model for the Right Task"

Tek bir model (One-size-fits-all) yerine, her gÃ¶revi o alanda en iyi olan ve *kendi baÄŸÄ±msÄ±z rate limitine sahip* modele yÃ¶nlendireceÄŸiz. Bu sayede `llama-3.3-70b` kotamÄ±z dolsa bile, kod yazma ve vizyon Ã¶zellikleri Ã§alÄ±ÅŸmaya devam edecek.

### Model Rol DaÄŸÄ±lÄ±mÄ±

| Rol | Birincil Model (Primary) | Yedek Model (Fallback) | Neden? |
| :--- | :--- | :--- | :--- |
| **ðŸ§  General Chat** | `llama-3.3-70b-versatile` | `llama-4-maverick-17b` | 70B en yÃ¼ksek EQ/IQ dengesine sahip. Llama 4 ise Ã§ok hÄ±zlÄ± ve zeki. |
| **ðŸ§® Logic & Math** | `qwen/qwen3-32b` | `openai/gpt-oss-120b` | Qwen 3 matematik/mantÄ±kta rakipsiz (%93.8). |
| **ðŸ’» Coding** | `qwen/qwen3-32b` | `moonshotai/kimi-k2-instruct` | Qwen kodlamada Ã§ok iyi. Kimi ise Ã§ok uzun kodlarÄ± okuyabilir. |
| **ðŸ‘ï¸ Vision (GÃ¶rsel)** | `llama-4-maverick-17b` | `llama-4-scout-17b` | Maverick native multimodal ve yÃ¼ksek detay baÅŸarÄ±sÄ± var. |
| **ðŸ“š Long Context** | `moonshotai/kimi-k2-instruct` | `openai/gpt-oss-120b` | Kimi 256K context ile kitap/belge analizi iÃ§in tek seÃ§enek. |
| **ðŸš€ Fast/Router** | `llama-4-scout-17b` | `llama-3.1-8b-instant` | Scout, 8b kadar hÄ±zlÄ± ama daha zeki (17B MoE). |
| **ðŸ›¡ï¸ Safety** | `llama-guard-4-12b` | `gpt-oss-safeguard-20b` | Guard 4 hem metin hem resim denetleyebilir. |

---

## 2. Rate Limit Orkestrasyonu

Her modelin dakikalÄ±k token (TPM) ve istek (RPM) limiti baÄŸÄ±msÄ±zdÄ±r. Bu mimariyi ÅŸÃ¶yle kullanacaÄŸÄ±z:

1.  **Paralel YÃ¼k DaÄŸÄ±tÄ±mÄ±:**
    *   KullanÄ±cÄ± kod sorduÄŸunda -> **Qwen 3** kotasÄ±ndan yer (Llama kotasÄ± etkilenmez).
    *   KullanÄ±cÄ± resim attÄ±ÄŸÄ±nda -> **Llama 4** kotasÄ±ndan yer.
    *   KullanÄ±cÄ± sohbet ettiÄŸinde -> **Llama 3.3** kotasÄ±ndan yer.
    
    *SonuÃ§:* Toplam kapasite 3-4 katÄ±na Ã§Ä±kar.

2.  **AkÄ±llÄ± Fallback Zinciri:**
    *   EÄŸer `llama-3.3-70b` 429 (Too Many Requests) hatasÄ± verirse:
        *   Sistem anÄ±nda `llama-4-maverick-17b` modeline geÃ§er.
        *   KullanÄ±cÄ± hissetmez, sadece cevap biraz daha kÄ±salabilir.

---

## 3. Uygulama PlanÄ±

### A. KonfigÃ¼rasyon GÃ¼ncellemesi (`config.py`)

```python
class Settings(BaseSettings):
    # ... mevcut ayarlar ...
    
    # ROL BAZLI MODEL TANIMLARI
    MODEL_CHAT_PRIMARY: str = "llama-3.3-70b-versatile"
    MODEL_CHAT_FALLBACK: str = "meta-llama/llama-4-maverick-17b-128e-instruct"
    
    MODEL_LOGIC_PRIMARY: str = "qwen/qwen3-32b"
    MODEL_LOGIC_FALLBACK: str = "openai/gpt-oss-120b"
    
    MODEL_VISION_PRIMARY: str = "meta-llama/llama-4-maverick-17b-128e-instruct"
    
    MODEL_LONG_CONTEXT: str = "moonshotai/kimi-k2-instruct-0905"
```

### B. AkÄ±llÄ± Model SeÃ§ici (`model_selector.py`)

Yeni bir servis (`app/chat/model_selector.py`) oluÅŸturulacak. `SmartRouter` sadece amacÄ± (intent) belirleyecek, `ModelSelector` ise o anki duruma ve kotaya gÃ¶re en iyi modeli seÃ§ecek.

**MantÄ±k:**
```python
def select_model(intent, domain, context_length):
    if context_length > 30000:
        return settings.MODEL_LONG_CONTEXT
        
    if intent == "vision":
        return settings.MODEL_VISION_PRIMARY
        
    if domain in ["math", "code", "logic"]:
        return settings.MODEL_LOGIC_PRIMARY
        
    return settings.MODEL_CHAT_PRIMARY
```

### C. Fallback DekoratÃ¶rÃ¼ (`decider.py`)

API Ã§aÄŸrÄ±larÄ±, model bazlÄ± fallback destekleyecek ÅŸekilde gÃ¼ncellenecek.

```python
FALLBACK_MAP = {
    settings.MODEL_CHAT_PRIMARY: settings.MODEL_CHAT_FALLBACK,
    settings.MODEL_LOGIC_PRIMARY: settings.MODEL_LOGIC_FALLBACK,
}

async def call_groq_safe(...):
    try:
        # Ana modeli dene
    except RateLimitError:
        # Fallback tablosuna bak ve yedek modeli dene
        backup_model = FALLBACK_MAP.get(current_model)
        if backup_model:
            # Yedek modelle tekrar dene
```

---

## 4. Ã–zet Faydalar

1.  **Kesintisiz Deneyim:** Bir model dursa bile diÄŸerleri Ã§alÄ±ÅŸÄ±r.
2.  **UzmanlÄ±k:** Matematik sorularÄ±nÄ± matematikÃ§iye (Qwen), sohbeti konuÅŸmacÄ±ya (Llama) yÃ¶nlendiririz.
3.  **Kapasite ArtÄ±ÅŸÄ±:** Groq'un sunduÄŸu toplam "Ã¼cretsiz" kapasiteyi sonuna kadar kullanÄ±rÄ±z.
