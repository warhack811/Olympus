# ğŸš€ Hybrid Model Orchestration Strategy (Limit-Aware Edition)

**Vizyon:** Benchmark sonuÃ§larÄ± ve **GÃ¼nlÃ¼k KullanÄ±m Limitleri (TPD/TPM)** analiz edilerek optimize edilmiÅŸ, "SÃ¼rdÃ¼rÃ¼lebilir ve YÃ¼ksek Zeka" mimarisi.

---

## ğŸ—ï¸ 4-KatmanlÄ± AkÄ±llÄ± Mimari (The 4-Tier Architecture)

Sistemi sadece "Zeka"ya gÃ¶re deÄŸil, "Maliyet ve Limit" verimliliÄŸine gÃ¶re 4 katmana ayÄ±rdÄ±k.

### ğŸŸ¡ Katman 0: KapÄ± BekÃ§isi (The Router) ğŸš¦
**Model:** `Meta Llama-4-Scout-17b`
*   **Kapasite:** 30K TPM (Ã‡ok HÄ±zlÄ±) / 500K TPD (YÃ¼ksek Hacim)
*   **GÃ¶revi:** Gelen isteÄŸi anÄ±nda analiz eder, zorluk puanÄ± verir (1-10) ve uygun katmana yÃ¶nlendirir.
*   **Neden?** En yÃ¼ksek anlÄ±k jeton iÅŸleme kapasitesine sahip. DarboÄŸaz yaratmaz.

### ğŸŸ¢ Katman 1: Vitrin (The Tongue) ğŸ—£ï¸
**Model:** `MoonshotAI Kimi-k2-Instruct`
*   **Yedek:** `Qwen3-32b`
*   **Kapasite:** 60 RPM (YÃ¼ksek Ä°stek HÄ±zÄ±) / 300K TPD
*   **GÃ¶revi:** KullanÄ±cÄ± ile sohbet, yaratÄ±cÄ± yazarlÄ±k, kÃ¼ltÃ¼rel iÃ§erik.
*   **Strateji:** GÃ¼nlÃ¼k sohbet yÃ¼kÃ¼nÃ¼ (300K TPD) bu model taÅŸÄ±r. Limit dolarsa Qwen devreye girer.

### ğŸ”µ Katman 2: Ä°ÅŸÃ§i ArÄ± (The Worker/Middle Brain) ğŸ
**Model:** `Meta Llama-4-Scout-17b`
*   **Kapasite:** 500K TPD (Devasa Hacim)
*   **GÃ¶revi:**
    *   Ã–zetleme (Summarization).
    *   Basit bilgi Ã§Ä±karma (Extraction).
    *   RAG Ã¶n iÅŸleme.
*   **Neden?** Zeki modellerin (GPT-OSS) kÄ±ymetli limitlerini "Ã¶zet Ã§Ä±karma" gibi basit iÅŸlerle harcamamak iÃ§in bu geniÅŸ kapasiteli modeli "Hamal" olarak kullanÄ±yoruz.

### ğŸ”´ Katman 3: AÄŸÄ±r Top (The Deep Brain) ğŸ§ 
**Model:** `OpenAI GPT-OSS-120b`
*   **Yedek:** `Llama-3.3-70b-Versatile`
*   **Kapasite:** 200K TPD (SÄ±nÄ±rlÄ±) / 8K TPM (YavaÅŸ)
*   **GÃ¶revi:**
    *   KarmaÅŸÄ±k Kodlama (Python/SQL).
    *   Zor MantÄ±k SorularÄ± (Reasoning).
    *   Sadece Router "Zorluk > 7" derse Ã§alÄ±ÅŸÄ±r.
*   **Strateji:** Llama-70B'nin 100K limiti Ã§ok dÃ¼ÅŸÃ¼k olduÄŸu iÃ§in onu sadece "Acil Durum YedeÄŸi" yaptÄ±k. GPT-OSS ana beyin.

---

## ğŸ› ï¸ YÃ¶nlendirme MantÄ±ÄŸÄ± (Routing Logic)

```python
def smart_route(user_query):
    # Llama-4-Scout ile analiz
    analysis = scout_classify(user_query)
    
    if analysis.type == "CHAT":
        return kimi_k2.generate(user_query) # Katman 1
        
    elif analysis.type == "TASK":
        if analysis.complexity < 5:
            return scout_17b.solve(user_query) # Katman 2 (Ucuz Ä°ÅŸÃ§i)
        else:
            return gpt_oss.solve(user_query) # Katman 3 (PahalÄ± Beyin)
```

---

## ğŸ“ˆ Neden Bu Mimari?

1.  **SÃ¼rdÃ¼rÃ¼lebilirlik:** Llama-70B'yi ana model yapsaydÄ±k (100K limit), sistem gÃ¼nde 50 kullanÄ±cÄ±dan sonra dururdu. Bu yapÄ±yla binlerce istek karÅŸÄ±lanabilir.
2.  **HÄ±z:** Router olarak 30K TPM'li Scout'u seÃ§mek, sistemin "DÃ¼ÅŸÃ¼nme SÃ¼resini" minimize eder.
3.  **GÃ¼venlik:** Her katmanÄ±n bir yedeÄŸi (Failover) vardÄ±r. Sistem asla "Hizmet DÄ±ÅŸÄ±" olmaz.
