"""
Mami AI - Groq API & Query Builder
==================================

Bu modÃ¼l Groq API Ã§aÄŸrÄ±larÄ± ve arama sorgusu Ã¼retimi yapar.

Sorumluluklar:
    - Groq API Ã§aÄŸrÄ±larÄ± (Ã§oklu anahtar rotasyonu)
    - INTERNET iÃ§in arama sorgusu Ã¼retimi
    - HafÄ±za kayÄ±t kararlarÄ±

KullanÄ±m:
    from app.chat.decider import call_groq_api_async, build_search_queries_async

    # Groq API Ã§aÄŸrÄ±sÄ±
    response = await call_groq_api_async(messages, model="llama-3.3-70b")

    # Ä°nternet aramasÄ± iÃ§in sorgu Ã¼retimi
    queries = await build_search_queries_async("Dolar kuru nedir?")
"""

from __future__ import annotations

import json
import logging
from collections.abc import AsyncGenerator
from typing import Any

# ModÃ¼l logger'Ä±
logger = logging.getLogger(__name__)

# --- LIVE TRACE ---
try:
    from app.core.live_tracer import LiveTracer
except ImportError:
    class LiveTracer:
        @staticmethod
        def warning(*args, **kwargs): pass
        @staticmethod
        def model_select(*args, **kwargs): pass
# ------------------

# =============================================================================
# YAPILANDIRMA
# =============================================================================

GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"
"""Groq API endpoint URL'si."""

DEFAULT_GROQ_TIMEOUT = 15.0
"""VarsayÄ±lan API zaman aÅŸÄ±mÄ± (saniye)."""


# =============================================================================
# LAZY IMPORTS & API KEY YÃ–NETÄ°MÄ°
# =============================================================================


def _get_settings():
    """AyarlarÄ± lazy import ile yÃ¼kler."""
    from app.config import get_settings
    return get_settings()


# =============================================================================
# LLM GENERATOR BRIDGE (Atlas Governance)
# =============================================================================
_GENERATOR = None


def _get_llm_generator():
    """Governance + KeyManager + Budget ile Groq adapter'Ä± dÃ¶ndÃ¼rÃ¼r."""
    global _GENERATOR
    if _GENERATOR is None:
        from app.core.llm import LLMGenerator
        from app.core.llm.adapters import groq_adapter

        _GENERATOR = LLMGenerator(providers={"groq": groq_adapter})
    return _GENERATOR


def get_available_keys() -> list[str]:
    """
    TÃ¼m geÃ§erli Groq API anahtarlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r. (Public Alias)
    """
    return _get_available_keys()

def _get_available_keys() -> list[str]:
    """
    TÃ¼m geÃ§erli Groq API anahtarlarÄ±nÄ± dÃ¶ndÃ¼rÃ¼r.

    BoÅŸ olmayan anahtarlar rotasyon iÃ§in sÄ±rayla denenir.
    """
    settings = _get_settings()
    keys = [
        settings.GROQ_API_KEY,
        settings.GROQ_API_KEY_BACKUP,
        settings.GROQ_API_KEY_4,
        getattr(settings, "GROQ_API_KEY_3", None),
    ]
    return [k for k in keys if k]


# =============================================================================
# GROQ API FONKSÄ°YONLARI
# =============================================================================


async def call_groq_api_async(
    messages: list[dict[str, str]],
    model: str | None = None,
    json_mode: bool = False,
    temperature: float = 0.7,
    timeout: float = DEFAULT_GROQ_TIMEOUT,
) -> str | None:
    """
    Groq Chat API Ã§aÄŸrÄ±sÄ± (async) - yeni LLMGenerator Ã¼zerinden governance/key rotasyonu.
    """
    generator = _get_llm_generator()
    from app.core.llm.generator import LLMRequest

    request = LLMRequest(
        role="decider",
        messages=messages,
        temperature=temperature,
        metadata={"override_model": model} if model else None,
    )
    result = await generator.generate(request)
    if result.ok:
        return result.text
    LiveTracer.warning("GROQ", f"Decider failed: {result.text}")
    return None


async def call_groq_api_safe_async(
    messages: list[dict[str, str]],
    model: str | None = None,
    json_mode: bool = False,
    temperature: float = 0.7,
    timeout: float = DEFAULT_GROQ_TIMEOUT,
    max_retries: int = 2,
) -> tuple[str | None, str | None]:
    """
    Retry mekanizmalÄ± ve Fallback zincirli gÃ¼venli Groq API Ã§aÄŸrÄ±sÄ±.
    
    1. Ä°stenen model (veya governance default) denenir (KeyManager rotasyonu ile).
    2. BaÅŸarÄ±sÄ±z olursa, FALLBACK_CHAINS yapÄ±landÄ±rmasÄ±ndaki alternatifler denenir.
    
    Args:
        messages: Mesaj listesi
        model: Model adÄ± (opsiyonel, governance'dan alÄ±nÄ±r)
    """
    settings = _get_settings()
    
    # Model belirtilmemiÅŸse governance'dan al
    if not model:
        from app.core.llm.governance import governance
        chain = governance.get_model_chain("synthesizer")
        primary_model = chain[0] if chain else "llama-3.3-70b-versatile"
    else:
        primary_model = model
    
    # 1. Denenecek Modelleri Belirle
    attempt_models = [primary_model]
    
    # Fallback zincirini ekle
    fallbacks = settings.FALLBACK_CHAINS.get(primary_model, settings.FALLBACK_CHAINS.get("default", []))
    for fm in fallbacks:
        if fm not in attempt_models:
            attempt_models.append(fm)
            
    last_error: str | None = None
    
    for active_model in attempt_models:
        if active_model != primary_model:
            logger.warning(f"[GROQ_SAFE] Model Fallback: {primary_model} -> {active_model}")
            
        # call_groq_api_async zaten kendi iÃ§inde Key Rotasyonu (5 deneme) yapÄ±yor.
        # Bu yÃ¼zden burada tekrar retry loop'a gerek yok, model deÄŸiÅŸtirmek daha mantÄ±klÄ±.
        
        content = await call_groq_api_async(
            messages=messages,
            model=active_model,
            json_mode=json_mode,
            temperature=temperature,
            timeout=timeout,
        )
        
        if content:
            # EÄŸer fallback ile yanÄ±t alÄ±ndÄ±ysa logla
            if active_model != primary_model:
                logger.info(f"[GROQ_SAFE] Fallback baÅŸarÄ±sÄ±: {active_model}")
            return content, None
            
        last_error = f"model_failed_{active_model}"

    logger.critical(f"[GROQ_SAFE] TÃœM MODEL ZÄ°NCÄ°RÄ° BAÅARISIZ! {attempt_models}")
    return None, "all_models_failed"


async def call_groq_api_stream_async(
    messages: list[dict[str, Any]],
    model: str | None = None,
    temperature: float = 0.7,
    timeout: float = DEFAULT_GROQ_TIMEOUT,
) -> AsyncGenerator[str, None]:
    """
    Streaming Groq API Ã§aÄŸrÄ±sÄ± (LLMGenerator Ã¼zerinden).
    """
    generator = _get_llm_generator()
    from app.core.llm.generator import LLMRequest

    request = LLMRequest(
        role="decider",
        messages=messages,
        temperature=temperature,
        metadata={"override_model": model} if model else None,
    )
    async for chunk in generator.generate_stream(request):
        yield chunk


# =============================================================================
# SÄ°STEM PROMPTLARI
# =============================================================================

# DECIDER_SYSTEM_PROMPT silindi - SmartRouter artÄ±k action belirliyor

MEMORY_DECIDER_SYSTEM_PROMPT = """
Sen Mami AI'Ä±n HafÄ±za YÃ¶neticisisin. GÃ¶revin, konuÅŸma akÄ±ÅŸÄ±ndan kullanÄ±cÄ±yla ilgili Ã–NEMLÄ° bilgileri yakalamak ve kategorize etmektir.

## 1. HAFIZA TÄ°PLERÄ° (YENÄ° SÄ°STEM ğŸ§ )
Bilgiyi yakalamadan Ã¶nce ÅŸu 4 kategoriden hangisine girdiÄŸine karar ver:

1.  **ğŸ” FACT (type="fact"):** KalÄ±cÄ±, deÄŸiÅŸmez gerÃ§ekler.
    *   *Ã–rnek:* "KullanÄ±cÄ± vegan", "KullanÄ±cÄ± 30 yaÅŸÄ±nda", "KullanÄ±cÄ± Ä°stanbul'da yaÅŸÄ±yor".
    *   *SÃ¼re:* KalÄ±cÄ±.

2.  **ğŸ“… EVENT (type="event"):** GeleceÄŸe yÃ¶nelik planlar, randevular veya zamanlÄ± iÅŸler.
    *   *Ã–rnek:* "YarÄ±n saat 15:00'te doktor randevusu var", "Haftaya tatile gidiyor".
    *   *SÃ¼re:* Olay gerÃ§ekleÅŸene kadar.

3.  **âš¡ STATE (type="state"):** GeÃ§ici durumlar, duygular veya anlÄ±k baÄŸlam.
    *   *Ã–rnek:* "KullanÄ±cÄ± bugÃ¼n yorgun hissediyor", "Åu an Python projesi debug ediyor".
    *   *SÃ¼re:* KÄ±sa vadeli (24-48 saat).

4.  **ğŸ—‘ï¸ NOISE (type="noise"):** Gereksiz bilgi. SAKLAMA.
    *   *Ã–rnek:* "Hava gÃ¼zel", "Selam naber", "TeÅŸekkÃ¼rler", Genel sohbet dolgusu, AI'Ä±n kendi cevaplarÄ±.

## 2. KARAR MEKANÄ°ZMASI (CHAIN OF THOUGHT)
Karar vermeden Ã¶nce ÅŸu adÄ±mlarÄ± izle:
1.  **Analiz:** KullanÄ±cÄ± ne dedi? Bu bilgi kiÅŸisel mi yoksa genel mi?
2.  **Kategori:** Fact, Event veya State mi? Yoksa Noise mu?
3.  **Ã–nem:** 0.0 (Gereksiz) ile 1.0 (Kritik) arasÄ±nda puan ver.
4.  **Kontrol:** Bu bilgi mevcut hafÄ±zayla Ã§eliÅŸiyor mu? (Ã–rn: "Evliyim" dedikten sonra "BekarÄ±m" demesi).

## 3. JSON Ã‡IKTI FORMATI
Kesinlikle sadece geÃ§erli bir JSON dÃ¶ndÃ¼r. Yorum yok.

```json
{
  "reasoning": "KullanÄ±cÄ± yarÄ±n iÃ§in bir plan belirtti, bu bir Event.",
  "store": true,
  "memory": "KullanÄ±cÄ±nÄ±n yarÄ±n saat 14:00'te toplantÄ±sÄ± var",
  "type": "event",
  "importance": 0.8,
  "topic": "schedule",
  "invalidate": []
}
```

EÄŸer SAKLANMAYACAKSA (Noise):
```json
{
  "reasoning": "KullanÄ±cÄ± sadece selam verdi, bilgi deÄŸeri yok.",
  "store": false
}
```

## Ã–NEMLÄ° KURALLAR:
*   MÃœMKÃœN OLDUÄUNCA AZ KAYIT AL. Sadece *gerÃ§ekten* iÅŸe yarayacak bilgileri sakla.
*   "type" alanÄ± zorunludur (fact, event, state).
*   GeÃ§ici duygularÄ± "state" olarak kaydet, "fact" yapma.
""".strip()

# RAG_DECIDER_SYSTEM_PROMPT ve CONVERSATION_SUMMARY_SYSTEM silindi - KullanÄ±lmÄ±yordu


# run_decider_async silindi - SmartRouter artÄ±k action belirliyor
# build_search_queries_async kullanÄ±lmalÄ±

# -----------------------------------------------------------------------------
# QUERY BUILDER (Secenek B - Yeni Sistem)
# -----------------------------------------------------------------------------

QUERY_BUILDER_PROMPT = """
You are a search query generator. Given a user's question, create 1-3 optimized web search queries.

Guidelines:
- For finance (dolar, euro, altÄ±n): Add "kuru bugÃ¼n gÃ¼ncel" to make it time-specific
- For weather: Add city name if mentioned + "hava durumu"
- For sports: Add team name + "son maÃ§ skor"
- For news: Add "son dakika" or "gÃ¼ncel haber"
- Keep queries in Turkish

Output JSON: {"queries": [{"id": "q1", "query": "..."}, {"id": "q2", "query": "..."}]}
"""


async def build_search_queries_async(message: str, semantic: dict[str, Any] | None = None) -> list[dict[str, str]]:
    """
    INTERNET akÄ±ÅŸÄ± iÃ§in arama sorgularÄ± Ã¼retir.

    Ã–zellikler:
    - SmartRouter'dan baÄŸÄ±msÄ±z Ã§alÄ±ÅŸÄ±r
    - Sadece sorgu oluÅŸturmaya odaklanÄ±r
    - Semantic analiz sonuÃ§larÄ±nÄ± kullanabilir

    Args:
        message: KullanÄ±cÄ± mesajÄ±
        semantic: Semantic analiz sonuÃ§larÄ± (opsiyonel)

    Returns:
        List[Dict]: [{"id": "q1", "query": "..."}]
    """
    # Domain bazlÄ± basit kontrol
    domain = semantic.get("domain", "general") if semantic else "general"
    text_lower = message.lower()

    # 1. HÄ±zlÄ± template kontrolÃ¼ (LLM Ã§aÄŸrÄ±sÄ± gerekmez)
    if domain == "finance" or any(kw in text_lower for kw in ["dolar", "euro", "altÄ±n", "kur"]):
        for currency in ["dolar", "euro", "altÄ±n", "sterlin"]:
            if currency in text_lower:
                return [{"id": "q1", "query": f"{currency} TL kuru bugÃ¼n gÃ¼ncel"}]

    if domain == "weather" or "hava" in text_lower:
        # Åehir Ã§Ä±karÄ±mÄ±
        cities = ["istanbul", "ankara", "izmir", "bursa", "antalya", "trabzon", "adana"]
        city = next((c for c in cities if c in text_lower), "tÃ¼rkiye")
        return [{"id": "q1", "query": f"{city} hava durumu"}]

    # 2. LLM ile akÄ±llÄ± sorgu Ã¼retimi
    llm_messages = [
        {"role": "system", "content": QUERY_BUILDER_PROMPT},
        {"role": "user", "content": message},
    ]

    content = await call_groq_api_async(llm_messages, json_mode=True, temperature=0.2)
    if content:
        try:
            data = json.loads(content)
            queries = data.get("queries", [])
            if queries:
                logger.info(f"[QUERY_BUILDER] LLM generated {len(queries)} queries")
                return queries
        except json.JSONDecodeError:
            logger.warning("[QUERY_BUILDER] JSON parse hatasÄ±, fallback'e geÃ§iliyor")

    # 3. Fallback: Ham mesajÄ± sorgu olarak kullan
    return [{"id": "q1", "query": message}]


async def decide_memory_storage_async(
    message: str, answer: str, existing_memories: list[dict[str, Any]] | None = None
) -> dict[str, Any]:
    """
    HafÄ±za kayÄ±t kararÄ±nÄ± LLM'e sorar.

    Conflict detection: Mevcut hafÄ±zalarla Ã§eliÅŸki varsa
    eski kayÄ±tlarÄ± invalidate eder.

    Args:
        message: KullanÄ±cÄ± mesajÄ±
        answer: Asistan yanÄ±tÄ±
        existing_memories: Mevcut ilgili hafÄ±zalar

    Returns:
        Dict: {store, memory, importance, category, invalidate}
    """
    existing_memories = existing_memories if existing_memories is not None else []

    # Mevcut hafÄ±zalarÄ± context olarak ekle
    memory_context = ""
    if existing_memories:
        memory_context = "\n\n## MEVCUT Ä°LGÄ°LÄ° HAFIZALAR:\n"
        for m in existing_memories:
            mid = m.get("id", "unknown")
            mtext = m.get("text", "")
            memory_context += f"- ID: {mid} | Text: {mtext}\n"

    user_content = f"KullanÄ±cÄ±: {message}\nAsistan: {answer}{memory_context}"

    messages = [
        {"role": "system", "content": MEMORY_DECIDER_SYSTEM_PROMPT},
        {"role": "user", "content": user_content},
    ]

    content = await call_groq_api_async(messages, json_mode=True, temperature=0.2)
    if content:
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            pass

    return {"store": False}


# decide_rag_storage_async ve summarize_conversation_for_rag_async silindi - HiÃ§ Ã§aÄŸrÄ±lmÄ±yordu
